#!/usr/bin/env python3
#
# This script provides an interface that combines the functions in the
# "BetaMining" package to analyze .pdb files of interest.
import prody
import numpy as np
import pandas as pd

from Bio.PDB.PDBParser import PDBParser
p = PDBParser(PERMISSIVE=1)

from sklearn.metrics import pairwise_distances

from biopandas.pdb import PandasPdb
from sys import exit
#import json
import gzip
from collections import defaultdict, deque
import os

def polymer_df(pdb_meta_dict, pdb_object):
  """returns a dataframe of polymer metadata and residue information from the
  PDB file. Returned dataframe consists of columns that identify the
  file, origin, and version of PDB prediction file the values are calculated from
  ("database", "id_code", "accession", "full_title", "depo_date") and metadata
  about the organism and protein the prediction file represents
  ("organism_taxid", "organism_scientific", "full_name", "fragment",
  "fragment_aa", "structure_aa", "aa_identity") and values for
  alpha-carbon coordinates ("x", "y", "z")

  Keyword arguments:
  pdb_meta_dict -- a dictionary of keys and values for dataframe columns
  pdb_file -- the filepath for the PDB file
  """
  df = pdb_object.loc[pdb_object["atom_name"] == "CA"]
  column_list_order = list(pdb_meta_dict.keys()) + ["residue_name", "residue_number", "x_coord", "y_coord", "z_coord", "b_factor"]
  df = df.assign(**pdb_meta_dict)
  df = df.reindex(columns = column_list_order)
  if pdb_meta_dict["full_title"].lower().find("alphafold") != -1:
      df = df.rename(columns = {"b_factor": "plddt"})
  #print(df)
  return df


def calculation_df(prody_model, residue_offset, secondary_structures, units = "degrees"):
  """returns a dataframe of relative residue numbers and phi, psi, omega, twist,
  and absolute residue numbers for a protein; returns a series of secondary
  structure symbols for

  Keyword arguments:
  prody_model -- a ProDy AtomGroup object
  pdb_meta_dict_offset -- the value for the residue offset to find the absolute
  residue number in a protein with multiple fragment PDB files
  """
  secondary_structure_dictionary = {}
  if units.lower()[0] == "d":
    radians = False
  elif units.lower()[0] == "r":
    radians = True

  symbol_list = []
  ramachandran_region_list = []
  data = []
  for residue in prody_model.iterResidues():
    symbol = "-"
    secondary_structure = "None"
    try:
      phi = prody.calcPhi(residue, radians, None)
      psi = prody.calcPsi(residue, radians, None)
      omega = prody.calcOmega(residue, radians, None)
      twist = psi - abs(phi)
      dihedrals_dictionary = {"phi": phi, "psi": psi, "omega": omega}
    except:
      phi, psi, omega, twist = pd.NA, pd.NA, pd.NA, pd.NA 
      continue
    else:
      for structure in secondary_structures:
        dihedrals = list(structure.keys() & dihedrals_dictionary.keys())
        if all(structure[d][units][0] <= dihedrals_dictionary[d] <= structure[d][units][1] for d in dihedrals) == True:
          secondary_structure = structure["name"]
          symbol = structure["symbol"]
          break

    finally:
      symbol_list.append(symbol)
      data.append((residue.getResnum(), int(residue.getResnum() + residue_offset), phi, psi, omega, twist, secondary_structure))
      if secondary_structure in secondary_structure_dictionary:
        secondary_structure_dictionary[secondary_structure].append(residue.getResnum())
      else:
        secondary_structure_dictionary[secondary_structure] = [residue.getResnum()]

  symbol_series = pd.Series(symbol_list)
  df = pd.DataFrame(data, columns = ["residue_number",
                                         "absolute_residue_number",
                                         "phi",
                                         "psi",
                                         "omega",
                                         "twist",
                                         "secondary_structure"])

  secondary_structure_dictionary["twist"] = dict(zip(df.residue_number, df.twist))
  #print(secondary_structure_dictionary)
  #print(symbol_series.to_string())
  return df, symbol_series, secondary_structure_dictionary

def contacts_df(pdb_dataframe, features_json, residue_features_dictionary, target_list):
  """returns a dataframe of residue numbers and a column of residues with which
  they are in contact according to target features, and a dictionary of boolean
  masks of which residues should be included or excluded from the final filter

  Keyword arguments:
  pdb_dataframe -- the dataframe generated using the "ATOM" entries in the PDB file
  features_json -- the dictionary generated from the "target_region_features" of the .json file
  residue_features_dictionary -- the dictionary generated by calculation_df,
  with key:value pairs of secondary structure and then lists of residue numbers
  that are assigned to that structure
  target_list -- a list of target region names for which to search the protein
  """
  coords_df = pdb_dataframe.loc[pdb_dataframe["atom_name"] == "CA"][["residue_number", "x_coord", "y_coord", "z_coord"]].reset_index(drop=True)
  residue_numbers_list = coords_df["residue_number"].values.tolist()
  distance_matrix = pairwise_distances(coords_df.loc[:,["x_coord", "y_coord", "z_coord"]], metric = "euclidean")

  #np.set_printoptions(threshold=sys.maxsize)
  #print(distance_matrix)
  
  column_names = ["residue_number"]

  series_mask_dictionary = {"include": [], "exclude": []}

  for target in features_json:
    if target["name"] in target_list:
      for condition in series_mask_dictionary.keys():
        if "contacts" in target[condition].keys():
          for contact_type in target[condition]["contacts"]:
            column_names.append(contact_type["name"])
            contact_max_distance = contact_type["max_distance"]
            contact_min_distance = contact_type["min_distance"]
            contact_flank = contact_type["excluded_flank"]
            contact_matrix = np.add(np.triu(distance_matrix, k = (1 + contact_flank)),np.tril(distance_matrix, k = -1 * (1 + contact_flank)))

            #print(residue_features_dictionary["beta-sheet"])
            #if contact_type["mask_symbol"] == "B":
              #print(contact_matrix[341])
            contact_matrix = np.where(contact_matrix > contact_max_distance, 0, contact_matrix)
            contact_matrix = np.where(contact_matrix < contact_min_distance, 0, contact_matrix)
            contact_matrix = np.where(contact_matrix > 0, 1, contact_matrix)

            #secondary - what it touches
            #target - what it was
            
            excluded_far_residue_indices = set()
            excluded_residue_indices = set()

            if "excluded_structure_flanks" in contact_type:
              for structure, flank_distance in contact_type["excluded_structure_flanks"].items():
                flank_distance = int(flank_distance)
                #print(structure, flank_distance)
                if structure in residue_features_dictionary:
                  #print(residue_features_dictionary[structure])
                  for i in residue_features_dictionary[structure]:
                    excluded_far_residue_indices.update(range(max(i - flank_distance, 0), min(i + flank_distance + 1, len(residue_numbers_list) - 1)))
                    #excluded_residue_indices.update(range(i - flank_distance, i + flank_distance + 1))
                  #print(excluded_far_residue_indices)

                  #excluded_far_residue_indices.extend(residue_features_dictionary[structure])
                  #excluded_residue_indices.extend(residue_features_dictionary[structure])

            #print(contact_type)
            #print(residue_features_dictionary)
            if "secondary_structures" in contact_type:
              #print(contact_type["secondary_structures"])
              for structure in contact_type["secondary_structures"]:
                if structure in residue_features_dictionary:
                  #print(residue_numbers_list)
                  #print(residue_features_dictionary[structure])
                  #print(residue_features_dictionary)
                  excluded_far_residue_indices.update({i - 1 for i in residue_numbers_list} - {i - 1 for i in residue_features_dictionary[structure]})
                  #print(excluded_far_residue_indices)
                else:
                  #The thing it would touch does not exist
                  contact_matrix[:,:] = 0

            if len(excluded_far_residue_indices) != 0:
              #excluded_far_residue_indices = list(np.asarray(list(set(residue_numbers_list) - excluded_far_residue_indices)) - 1)
              #print(excluded_far_residue_indices)
              contact_matrix[:,sorted(excluded_far_residue_indices)] = 0
              
              #with the flank addition - we want to increase the bounds of residues_contacted

            if contact_type["target_name"] != ["all"]:
              for label in contact_type["target_name"]:
                if label in residue_features_dictionary:
                  excluded_residue_indices.update(residue_features_dictionary[label])

            if len(excluded_residue_indices) != 0:
              excluded_residue_indices = list(np.asarray(list(set(residue_numbers_list) - excluded_residue_indices)) - 1)
              contact_matrix[sorted(excluded_residue_indices),:] = 0

            #print(excluded_residue_indices)
            #print(excluded_far_residue_indices)
            #if contact_type["mask_symbol"] == "B":
              #print(contact_matrix[341])

            target_matrix = contact_matrix
              #target_matrix[sorted(excluded_residue_indices),:] = 0
              #with the flank addition - we want to increase the bounds of excluded_residue_indices
            #else:
            #  target_matrix = contact_matrix

          contacts_df = pd.DataFrame(data = contact_matrix, columns = residue_numbers_list)
          coords_df[contact_type["name"]] = contacts_df.apply(lambda row: row[row == 1.0].index.tolist(), axis = 1)
          series_mask_dictionary[condition].append((pd.Series(np.sum(target_matrix, axis = 1).astype(bool)),contact_type["mask_symbol"]))

  contacts_dataframe = coords_df[column_names]
  return contacts_dataframe, series_mask_dictionary

def parseDSSP(dsspFile):
  """Returns sheetIDs and strand ranges from dssp output.
  This function captures the name of the sheet a strand is in, which is missed in most parsers,
  and doubles the possible sheetIDs.
  Requires the name of the dssp output.
  """
  RESIDUE_INDEXES = (0,5)
  STRUCTURE_INDEX = 16
  SHEET_INDEX = 33
  MAX_DEQUE_SIZE = 10
  
  sheetStrandDict = defaultdict(list) #key=sheetID (eg. "A"): value=[(strandMin, strandMax), ...]
  #note about dssp sheetIDs - they are A-Z, but loop around if there are more than 26 sheets in one pdb
  #this can be the case for large/complex proteins
  #the workaround I used was to make the sheetID lowercases after it loops
  #this allows for 52 sheets, which is better, but not perfect.
  #a better method would be to assign an integer sheetID - maybe do that at some point?
  #it's unlikely to change anything meaningfully
  
  with open(dsspFile) as inFile:
    inHeader = True
    while inHeader:
      line = inFile.readline()
      if " #  RESIDUE" in line:
        inHeader = False
                                                
    currentStrandMinIndex = 0
    currentStrandMaxIndex = 0
    currentSheet = ""
    recentSheets = deque() #holds sheetIDs, use appendleft and pop
    for line in inFile:
      secondaryStructure = line[STRUCTURE_INDEX]
      if secondaryStructure == "E":
        residue = int(line[RESIDUE_INDEXES[0]:RESIDUE_INDEXES[1]])
        currentStrandMinIndex = residue if currentStrandMinIndex == 0 else currentStrandMinIndex
        currentStrandMaxIndex = residue
        sheet = line[SHEET_INDEX]
        currentSheet = sheet if currentSheet == "" else currentSheet
        if sheet != currentSheet:
          if currentSheet not in recentSheets:
            if currentSheet.lower() in recentSheets:
              currentSheet = currentSheet.lower()
            if "Z" in sheetStrandDict:
              currentSheet = currentSheet.lower()
            if currentSheet not in recentSheets:
              recentSheets.appendleft(currentSheet)
            if len(recentSheets) > MAX_DEQUE_SIZE:
              recentSheets.pop()
          sheetStrandDict[currentSheet].append((currentStrandMinIndex, currentStrandMaxIndex - 1)) #max-1 because this is first of next strand
          currentStrandMinIndex = residue
          currentStrandMaxIndex = 0
          currentSheet = sheet
      else: #residue is not in a strand
        if currentStrandMaxIndex != 0:
          if currentSheet not in recentSheets:
            if currentSheet.lower() in recentSheets:
              currentSheet = currentSheet.lower()
            if "Z" in sheetStrandDict:
              currentSheet = currentSheet.lower()
            if currentSheet not in recentSheets:
              recentSheets.appendleft(currentSheet)
            if len(recentSheets) > MAX_DEQUE_SIZE:
              recentSheets.pop()
          sheetStrandDict[currentSheet].append((currentStrandMinIndex, currentStrandMaxIndex))
          currentStrandMinIndex = 0
          currentStrandMaxIndex = 0
          currentSheet = ""
  return sheetStrandDict

def sheetsAreGood(interactingStrandAvgs, max_degrees):
  """Returns True if all strands in nearby sheets are in roughly the same plane.
  Based on strand-pair wise angle differences.
  """
  for i in range(len(interactingStrandAvgs)):
    for j in range(i + 1, len(interactingStrandAvgs)):
      angle = np.arccos(np.clip(np.dot(interactingStrandAvgs[i], interactingStrandAvgs[j]), -1.0, 1.0))
      angle = angle * (180 / 3.1415)
      if not (-1 * max_degrees) <= angle <= max_degrees:
        if not (-1 * max_degrees) <= (angle - 180) <= max_degrees:
          #reject all residues in ALL of these sheets
          return False
  return True

def handle_out_of_plane(model, options, filename, out_path, thread_id):
  """determines which residues are in out of plane sheets based on the criteria in the json
  and returns a series_mask in the style of series_mask_dictionary from contacts_df.
  This also runs dssp on the pdb - generating a temp.dssp file.
  """
  min_residues = options["min_residues_for_comp"]
  max_degrees = options["max_degrees_difference"]
  max_distance = options["max_distance"]
  symbol = options["mask_symbol"]
  series_mask = pd.Series([False for _ in range(model.numResidues())])

  #tempDSSPFileBase = out_path + "temp" + "_" + str(thread_id)
  tempDSSPFileBase = f"{out_path}temp_{thread_id}"
  #print(tempDSSPFileBase)
  
  #This is included just in case it happens. I don't think it will, but I don't entirely trust current_process()
  if os.path.isfile(tempDSSPFileBase + ".dssp"):
    exit("\n\nTemp files overlapped - the thread_id method needs to be changed.\n\n")

  if not filename.endswith(".gz"):
    prody.execDSSP(filename, tempDSSPFileBase)
  else:
    tempPDBFile = out_path + "temp_" + str(thread_id) + ".pdb"
    with gzip.open(filename, "rt") as inFile, open(tempPDBFile, "wt") as outFile:
      outFile.write(inFile.read())
    
    prody.execDSSP(tempPDBFile, tempDSSPFileBase)

    if os.path.isfile(tempPDBFile):
      os.remove(tempPDBFile)

  sheetStrandDict = parseDSSP(tempDSSPFileBase + ".dssp")

  if os.path.isfile(tempDSSPFileBase + ".dssp"):
    os.remove(tempDSSPFileBase + ".dssp")

  if len(sheetStrandDict) == 0:
    return(series_mask, symbol)

  cCoords = np.array(model.select("name C").getCoords())
  oCoords = np.array(model.select("name O").getCoords())
  caCoords = np.array(model.ca.getCoords())
  coVectors = oCoords - cCoords
  #coVectors are the carbonyl vectors (from C to O), and are correct above.
  #Flip every other coVector so that they point to the same side of beta sheets
  #Note: this would cause major issues for any other structure.
  coVectors[::2] = -1 * coVectors[::2]


  sheetStrandAvgVectors = {sheetID: [] for sheetID in sheetStrandDict.keys()} #key=sheetID: value=[(avg x,y,z for sheet), ...]
  residueToSheetDict = {} #key=residueID: value=sheetID
  allSheetStrandResidues = [] #all residues assigned to a sheet
  for sheetID, strandRanges in sheetStrandDict.items():
    for strandRange in strandRanges:            
      allSheetStrandResidues.extend(list(range(strandRange[0] - 1, strandRange[1])))
      if (strandRange[1] - strandRange[0]) + 1 > min_residues: 
        sheetStrandAvgVectors[sheetID].append(np.mean(coVectors[strandRange[0] - 1: strandRange[1]], axis = 0))
      for residueID in range(strandRange[0] - 1, strandRange[1]):
        residueToSheetDict[residueID] = sheetID
  #change vectors to unit vectors
  sheetStrandAvgVectors = {sheetID: [vector / np.linalg.norm(vector) for vector in vectors] for sheetID, vectors in sheetStrandAvgVectors.items()}

  distances = pairwise_distances(caCoords[allSheetStrandResidues,], metric="euclidean")
  contacts = np.asarray(distances < max_distance) 
  allSheetStrandResidues = np.array(allSheetStrandResidues)

  sheetsInteracting = {}#key=sheetID:value={sheetID, sheetID,...} 
  for testIndex, residueContacts in enumerate(contacts):
    residueIndex = allSheetStrandResidues[testIndex]
    contactIndexes = np.flatnonzero(residueContacts)
    sheetID = residueToSheetDict[residueIndex]
    if sheetID not in sheetsInteracting:
      sheetsInteracting[sheetID] = set()
    for contactID in contactIndexes:
      contactResidue = allSheetStrandResidues[contactID]
      sheetsInteracting[sheetID].add(residueToSheetDict[contactResidue])

  rejectedResidues = set()
  for sheetList in sheetsInteracting.values():
    interactingStrandAvgs = [sheetStrandAvgVectors[sheetID] for sheetID in sheetList]
    #this is a 2-D list, made 1-D below
    interactingStrandAvgs = [vector for strandVectors in interactingStrandAvgs for vector in strandVectors]
    if not sheetsAreGood(interactingStrandAvgs, max_degrees):
      for sheetID in sheetList:
        for strandRange in sheetStrandDict[sheetID]:
          rejectedResidues.update(list(range(strandRange[0] - 1, strandRange[1])))

  series_mask[list(rejectedResidues)] = True

  return (series_mask, symbol)

def handle_flank(flank_json, match_object, dihedral_dataframe):
  """returns a boolean value indicating whether a regex match should be passed
  based on attributes of the flanking regions. The only attribute will only be the ratio of
  confidence scores (plddt) for now.

  Keyword arguments:
  flank_json -- only the section of the json dictionary which defines flank_confidence
  match_object -- the match object from the regex search being queried
  dihedral_dataframe -- the dataframe containing the calculated dihedral angles and other attributes for the protein
  """
  flank_size = flank_json["size"]
  use_first_flank = True 
  use_second_flank = True
  if match_object.span()[0] + 1 - flank_size <= 0:
    #print("not using first")
    use_first_flank = False
  if match_object.span()[1] + 2 + flank_size >= max(dihedral_dataframe.loc[:, "residue_number"]):
    #print("not using second")
    use_second_flank = False
  if use_first_flank == False and use_second_flank == False:
    print("Both flanks contained the terminus, but that might be fine")
    return True

  flank_df = pd.DataFrame(columns=dihedral_dataframe.columns) #init empty df, fill in next blocks
  if use_first_flank:
    flank_df = pd.concat([flank_df, dihedral_dataframe[dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1 - flank_size , match_object.span()[0] + 1)])]])
  if use_second_flank:
    flank_df = pd.concat([flank_df, dihedral_dataframe[dihedral_dataframe["residue_number"].isin([*range(match_object.span()[1] + 2, match_object.span()[1] + 2 + flank_size)])]])

  if "mean_ratio" in flank_json:
    ratio_min = flank_json["mean_ratio"][0]
    ratio_max = flank_json["mean_ratio"][1]
    
    in_region_mean = pd.Series.mean(dihedral_dataframe["plddt"][dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1, match_object.span()[1] + 2)])])
    flank_mean = pd.Series.mean(flank_df.loc[:, "plddt"]) 
    conf_ratio = flank_mean / in_region_mean

    if not (ratio_min <= conf_ratio <= ratio_max):
      print(f"Failed because the ratio was {conf_ratio} and needed to be between {ratio_min} and {ratio_max}")
      return False
  if "mean" in flank_json:
    flank_mean = pd.Series.mean(flank_df.loc[:, "plddt"]) 
    mean_min = flank_json["mean"][0]
    mean_max = flank_json["mean"][1]

    if not (mean_min <= flank_mean <= mean_max):
      print(f"Failed because the mean flank pLDDT was {flank_mean} and needed to be between {mean_min} and {mean_max}")
      return False
  return True

def attribute_filter(target_json, match_object, dihedral_dataframe):
  """returns a boolean value indicating whether a regex match should be passed
  (True) or failed (False) based on various qualities of the residue range

  Keyword arguments:
  target_json -- the target motif attribute section of the features .json
  match_object -- the match object from the regular expression search being queried
  dihedral_dataframe -- the dataframe containing the calculated dihedral angles and other attributes for the protein
  """
  available_calculation_list = list(dihedral_dataframe.columns)
  #print(available_calculation_list)
  for condition in ["exclude", "include"]: # the function immediately returns False if any part fails, so exclude comes first no matter what the order is in the YAML config
    for attribute in target_json[condition]:
      if attribute in available_calculation_list:
        for func_name in target_json[condition][attribute]:
          func = getattr(pd.Series, func_name)
          attribute_range_min = target_json[condition][attribute][func_name][0]
          attribute_range_max = target_json[condition][attribute][func_name][1]
          attribute_value = func(dihedral_dataframe[attribute][dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1, match_object.span()[1] + 2)])])
          #print(attribute_range_min) 
          #print(attribute_value) 
          #print(attribute_range_max)
          if attribute_range_min <= attribute_value <= attribute_range_max:
            if condition == "exclude":
              print(f"Failed because the {attribute} was {attribute_value} and needed to be between {attribute_range_min} and {attribute_range_max}")
              return False
          else:
            if condition == "include":
              print(f"Failed because the {attribute} was {attribute_value} and needed to be between {attribute_range_min} and {attribute_range_max}")
              return False
      elif attribute == "flank_plddt":
        if not handle_flank(target_json[condition][attribute], match_object, dihedral_dataframe):
          #print statements are in handle_flank
          return False
      elif attribute == "size":
        size_range_min = target_json[condition][attribute][0]
        size_range_max = target_json[condition][attribute][1]
        size = (match_object.span()[1] + 2) - (match_object.span()[0] + 1)
        if condition == "exclude":
          if (size_range_min <= size <= size_range_max):
            print(f"Failed because the size was {size} and couldn't be between {size_range_min} and {size_range_max}")
            return False
        if condition == "include":
          if not (size_range_min <= size <= size_range_max):
            print(f"Failed because the size was {size} and needed to be between {size_range_min} and {size_range_max}")
            return False
            

  return True

def attribute_calculations(config_add_attr, residue_range, poly_dataframe):
    """returns a list to be column values of calculated attributes of hit sequences based on the "additional_attributes" section of the YAML config file.

    Keyword arguments:
    config_add_attr -- dictionary calculated attributes to include from the "additional_attributes" section of the config YAML
    residue_range -- a tuple of the residue range over which to calculate the value
    poly_dataframe -- a dataframe with all per-residue values needed for the attribute calculations
    """
    pass

def parse_pdb_header_custom(pdb_file):
  """This function will handle cases where the accession id of a protein is 10 characters long.
  In this case, ProDy will fail to parse the header.
  This function will return a dictionary identical to what would be in meta_dictionary.
  """
  #database, idcode, accession, full_name, full_title, depo_date, fragment, fragment_offset, organism_scientific, organism_taxid
  meta_dictionary = {"database": None,
                     "id_code": None,
                     "accession": None,
                     "full_name": None,
                     "full_title": None,
                     "depo_date": None,
                     "fragment": None,
                     "fragment_offset": None,
                     "organism_scientific": None,
                     "organism_taxid": None}
  DBREF = {
    "GB": "GenBank",
    "PDB": "PDB",
    "UNP": "UniProt",
    "NORINE": "Norine",
    "UNIMES": "UNIMES",
    "EMDB": "EMDB"
  }

  if pdb_file.endswith(".gz"):
    fileHandle = gzip.open(pdb_file, "rt")
  else:
    fileHandle = open(pdb_file)
  title_list = []
  for line in fileHandle:
    if line.startswith("HEADER"):
      #last portion of HEADER line
      meta_dictionary["depo_date"] = line.strip().split()[-1]

    if line.startswith("TITLE"):
      #Remove starting stuff, which includes an int if not the first row,
      #put in list, join at the end
      if len(title_list) > 0:
        title_list.append(" ".join(line.strip().split()[2:]))
      else:
        title_list.append(" ".join(line.strip().split()[1:]))

    if line.startswith("COMPND"):
      #if "MOL_ID" in line:
        #the pdb definition says this is correct, but our usage of 'fragment' is different
        #split on whitespace, get last element, remove semicolon
        #meta_dictionary["fragment"] = line.strip().split()[-1][:-1]
      if "MOLECULE" in line:
        #split on colon and get everything after MOLECULE:, 
        #join back together with colon in case one appears in name
        #remove bordering whitespace and trailing semicolon
        meta_dictionary["full_name"] = ":".join(line.split(":")[1:]).strip(" ;\n")

    if line.startswith("SOURCE"):
      if "ORGANISM_SCIENTIFIC" in line:
        #split on colon to get everything after ORGANISM_SCIENTIFIC:,
        #join together with colon, remove trailing semicolon and any bordering whitespace
        meta_dictionary["organism_scientific"] = ":".join(line.strip().split(":")[1:]).strip(" ;\n")
      if "ORGANISM_TAXID" in line:
        meta_dictionary["organism_taxid"] = ":".join(line.strip().split(":")[1:]).strip(" ;\n")

    if line.startswith("DBREF"):
      line = line.strip().split()
      meta_dictionary["database"] = DBREF[line[5]]
      meta_dictionary["accession"] = line[6]
      meta_dictionary["id_code"] = line[7]
      meta_dictionary["fragment_offset"] = int(line[8]) - 1

  #This calculation works for pdbs from the alphafold db, where they shift the start of a fragment by 200aa.
  #This might need to be changed if a different source is used.
  meta_dictionary["fragment"] = (meta_dictionary["fragment_offset"] // 200) + 1
  meta_dictionary["full_title"] = " ".join(title_list)


  #print(meta_dictionary)

  fileHandle.close()
  return meta_dictionary

def create_model_metainfo(pdb_file):
    """returns a ProDy model and a dictionary of meta information about a protein, taken from the PDB file header. Also returns the amino acid sequence as a list of 1-letter codes, and a Pandas dataframe of atomic coordinates.

    Keyword arguments:
    pdb_file -- the PDB structure file
    """
    meta_dictionary = None
    if len(pdb_file.split("-")[1]) == 10:
      #if the accesssion is 10 characters (allowed since ~2014), ProDy will fail to parse the header
      meta_dictionary = parse_pdb_header_custom(pdb_file)
      model = prody.parsePDB(pdb_file, header=False, model=1, meta=True)
    else:
      try:
        model, pdb_head = prody.parsePDB(pdb_file, header=True, model=1, meta=True)
      except Exception:
        #print(traceback.format_exc())
        print("Prody failed, skipping this protein")
        return None
    # using BioPandas to get matrix of PDB values and convert sequence to single letter
    af_object = PandasPdb().read_pdb(pdb_file).df["ATOM"]
    af_sequence = list(PandasPdb().read_pdb(pdb_file).amino3to1()["residue_name"])

    # decompress if needed
    if pdb_file.endswith(".gz") == True:
        decompressed_pdb_file = gzip.open(pdb_file, "rt")
        bio_pdb = p.get_structure("XXXX", decompressed_pdb_file)
    else:
        bio_pdb = p.get_structure("XXXX", pdb_file)

    if meta_dictionary is None:
      meta_dictionary = {
          "database": pdb_head["polymers"][0].dbrefs[0].database,
          "id_code": pdb_head["polymers"][0].dbrefs[0].idcode,
          "accession": pdb_head["polymers"][0].dbrefs[0].accession,
          "full_name": pdb_head["polymers"][0].name,
          "full_title": pdb_head["title"],
          "depo_date": pdb_head["deposition_date"],
          #This calculation works for pdbs from the alphafold db, where they shift the start of a fragment by 200aa.
          #This might need to be changed if a different source is used.
          "fragment": int((((pdb_head["polymers"][0].dbrefs[0].first[2] - pdb_head["polymers"][0].dbrefs[0].first[0]) / 200)) + 1),
          "fragment_offset": pdb_head["polymers"][0].dbrefs[0].first[2] - pdb_head["polymers"][0].dbrefs[0].first[0],
          "organism_scientific": bio_pdb.header["source"]["1"]["organism_scientific"].upper(),
          "organism_taxid": bio_pdb.header["source"]["1"]["organism_taxid"]
          }
    #print(meta_dictionary)
    #pdb_file.close()

    return model, meta_dictionary, af_object, af_sequence
