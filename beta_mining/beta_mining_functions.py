#!/usr/bin/env python3
#
# This script provides an interface that combines the functions in the
# "BetaMining" package to analyze .pdb files of interest.
import prody
import numpy as np
import pandas as pd

from Bio.PDB.PDBParser import PDBParser
p = PDBParser(PERMISSIVE=1)

from sklearn.metrics import pairwise_distances

from biopandas.pdb import PandasPdb
import datetime
import os
import sys
import math
import re
import json
import os
import tarfile
import gzip
import shutil
import glob
from pathlib import Path
import traceback

#import beta_mining
#from beta_mining import beta_mining_algorithm
#from beta_mining import beta_mining_functions

def polymer_df(pdb_meta_dict, pdb_object):
  """returns a dataframe of polymer metadata and residue information from the
  PDB file. Returned dataframe consists of columns that identify the
  file, origin, and version of PDB prediction file the values are calculated from
  ("database", "id_code", "accession", "full_title", "depo_date") and metadata
  about the organism and protein the prediction file represents
  ("organism_taxid", "organism_scientific", "full_name", "fragment",
  "fragment_aa", "structure_aa", "aa_identity") and values for
  alpha-carbon coordinates ("x", "y", "z")

  Keyword arguments:
  pdb_meta_dict -- a dictionary of keys and values for dataframe columns
  pdb_file -- the filepath for the PDB file
  """
  df = pdb_object.loc[pdb_object["atom_name"] == "CA"]
  column_list_order = list(pdb_meta_dict.keys()) + ["residue_name", "residue_number", "x_coord", "y_coord", "z_coord", "b_factor"]
  df = df.assign(**pdb_meta_dict)
  df = df.reindex(columns = column_list_order)
  if pdb_meta_dict["full_title"].lower().find("alphafold") != -1:
      df = df.rename(columns = {"b_factor": "plddt"})
  #print(df)
  return df


def calculation_df(prody_model, residue_offset, secondary_structures, units = "degrees"):
  """returns a dataframe of relative residue numbers and phi, psi, omega, twist,
  and absolute residue numbers for a protein; returns a series of secondary
  structure symbols for

  Keyword arguments:
  prody_model -- a ProDy AtomGroup object
  pdb_meta_dict_offset -- the value for the residue offset to find the absolute
  residue number in a protein with multiple fragment PDB files
  """
  secondary_structure_dictionary = {}
  if units.lower()[0] == "d":
    radians = False
  elif units.lower()[0] == "r":
    radians = True

  symbol_list = []
  ramachandran_region_list = []
  data = []
  for residue in prody_model.iterResidues():
    symbol = "-"
    secondary_structure = "None"
    try:
      phi = prody.calcPhi(residue, radians, None)
      psi = prody.calcPsi(residue, radians, None)
      omega = prody.calcOmega(residue, radians, None)
      twist = psi - abs(phi)
      dihedrals_dictionary = {"phi": phi, "psi": psi, "omega": omega}
    except:
      phi, psi, omega, twist = pd.NA, pd.NA, pd.NA, pd.NA 
      continue
    else:
      for structure in secondary_structures:
        dihedrals = list(structure.keys() & dihedrals_dictionary.keys())
        if all(structure[d][units][0] <= dihedrals_dictionary[d] <= structure[d][units][1] for d in dihedrals) == True:
          secondary_structure = structure["name"]
          symbol = structure["symbol"]
          break

    finally:
      symbol_list.append(symbol)
      data.append((residue.getResnum(), int(residue.getResnum() + residue_offset), phi, psi, omega, twist, secondary_structure))
      if secondary_structure in secondary_structure_dictionary:
        secondary_structure_dictionary[secondary_structure].append(residue.getResnum())
      else:
        secondary_structure_dictionary[secondary_structure] = [residue.getResnum()]

  symbol_series = pd.Series(symbol_list)
  df = pd.DataFrame(data, columns = ["residue_number",
                                         "absolute_residue_number",
                                         "phi",
                                         "psi",
                                         "omega",
                                         "twist",
                                         "secondary_structure"])

  secondary_structure_dictionary["twist"] = dict(zip(df.residue_number, df.twist))
  #print(secondary_structure_dictionary)
  return df, symbol_series, secondary_structure_dictionary

def contacts_df(pdb_dataframe, features_json, residue_features_dictionary, target_list):
  """returns a dataframe of residue numbers and a column of residues with which
  they are in contact according to target features, and a dictionary of boolean
  masks of which residues should be included or excluded from the final filter

  Keyword arguments:
  pdb_dataframe -- the dataframe generated using the "ATOM" entries in the PDB file
  features_json -- the dictionary generated from the "target_region_features" of the .json file
  residue_features_dictionary -- the dictionary generated by calculation_df,
  with key:value pairs of secondary structure and then lists of residue numbers
  that are assigned to that structure
  target_list -- a list of target region names for which to search the protein
  """
  coords_df = pdb_dataframe.loc[pdb_dataframe["atom_name"] == "CA"][["residue_number", "x_coord", "y_coord", "z_coord"]].reset_index(drop=True)
  residue_numbers_list = coords_df["residue_number"].values.tolist()
  distance_matrix = pairwise_distances(coords_df, metric = "euclidean")
  column_names = ["residue_number"]

  series_mask_dictionary = {"include": [], "exclude": []}

  for target in features_json:
    if target["name"] in target_list:
      for condition in series_mask_dictionary.keys():
        if "contacts" in target[condition].keys():
          for contact_type in target[condition]["contacts"]:
            column_names.append(contact_type["name"])
            contact_max_distance = contact_type["max_distance"]
            contact_min_distance = contact_type["min_distance"]
            contact_flank = contact_type["excluded_flank"]
            contact_matrix = np.add(np.triu(distance_matrix, k = (1 + contact_flank)),np.tril(distance_matrix, k = -1 * (1 + contact_flank)))
            contact_matrix = np.where(contact_matrix > contact_max_distance, 0, contact_matrix)
            contact_matrix = np.where(contact_matrix < contact_min_distance, 0, contact_matrix)
            contact_matrix = np.where(contact_matrix > 0, 1, contact_matrix)
            if "secondary_structures" in contact_type:
              array_idx = []
              for structure in contact_type["secondary_structures"]:
                #print(structure)
                #print(residue_features_dictionary)
                if structure in residue_features_dictionary:
                  array_idx.extend(residue_features_dictionary[structure])
              array_idx = list(np.asarray(list(set(residue_numbers_list) - set(array_idx))) - 1)
              contact_matrix[:,array_idx] = 0
            if contact_type["target_name"] != ["all"]:
              target_idx = []
              for label in contact_type["target_name"]:
                #print(label)
                if label in residue_features_dictionary:
                  target_idx.extend(residue_features_dictionary[label])
              target_idx = list(np.asarray(list(set(residue_numbers_list) - set(target_idx))) - 1)
              target_matrix = contact_matrix
              target_matrix[array_idx,:] = 0
            else:
              target_matrix = contact_matrix

          contacts_df = pd.DataFrame(data = contact_matrix, columns = residue_numbers_list)
          coords_df[contact_type["name"]] = contacts_df.apply(lambda row: row[row == 1.0].index.tolist(), axis = 1)
          series_mask_dictionary[condition].append((pd.Series(np.sum(target_matrix, axis = 1).astype(bool)),contact_type["mask_symbol"]))

  contacts_dataframe = coords_df[column_names]
  return contacts_dataframe, series_mask_dictionary

def handle_flank(flank_json, match_object, dihedral_dataframe):
  """returns a boolean value indicating whether a regex match should be passed
  based on attributes of the flanking regions. The only attribute will only be the ratio of
  confidence scores (plddt) for now.

  Keyword arguments:
  flank_json -- only the section of the json dictionary which defines flank_confidence
  match_object -- the match object from the regex search being queried
  dihedral_dataframe -- the dataframe containing the calculated dihedral angles and other attributes for the protein
  """
  flank_size = flank_json["size"]
  use_first_flank = True 
  use_second_flank = True
  if match_object.span()[0] + 1 - flank_size <= 0:
    #print("not using first")
    use_first_flank = False
  if match_object.span()[1] + 2 + flank_size >= max(dihedral_dataframe.loc[:, "residue_number"]):
    #print("not using second")
    use_second_flank = False
  if use_first_flank == False and use_second_flank == False:
    print("Both flanks contained the terminus, but that might be fine")
    return True

  flank_df = pd.DataFrame(columns=dihedral_dataframe.columns) #init empty df, fill in next blocks
  if use_first_flank:
    flank_df = pd.concat([flank_df, dihedral_dataframe[dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1 - flank_size , match_object.span()[0] + 1)])]])
  if use_second_flank:
    flank_df = pd.concat([flank_df, dihedral_dataframe[dihedral_dataframe["residue_number"].isin([*range(match_object.span()[1] + 2, match_object.span()[1] + 2 + flank_size)])]])

  if "mean_ratio" in flank_json:
    ratio_min = flank_json["mean_ratio"][0]
    ratio_max = flank_json["mean_ratio"][1]
    
    in_region_mean = pd.Series.mean(dihedral_dataframe["plddt"][dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1, match_object.span()[1] + 2)])])
    flank_mean = pd.Series.mean(flank_df.loc[:, "plddt"]) 
    conf_ratio = flank_mean / in_region_mean

    if not (ratio_min <= conf_ratio <= ratio_max):
      print(f"Failed because the ratio was {conf_ratio} and needed to be between {ratio_min} and {ratio_max}")
      return False
  if "mean" in flank_json:
    flank_mean = pd.Series.mean(flank_df.loc[:, "plddt"]) 
    mean_min = flank_json["mean"][0]
    mean_max = flank_json["mean"][1]

    if not (mean_min <= flank_mean <= mean_max):
      print(f"Failed because the mean flank pLDDT was {flank_mean} and needed to be between {mean_min} and {mean_max}")
      return False
  return True

def attribute_filter(target_json, match_object, dihedral_dataframe):
  """returns a boolean value indicating whether a regex match should be passed
  (True) or failed (False) based on various qualities of the residue range

  Keyword arguments:
  target_json -- the target motif attribute section of the features .json
  match_object -- the match object from the regular expression search being queried
  dihedral_dataframe -- the dataframe containing the calculated dihedral angles and other attributes for the protein
  """
  available_calculation_list = list(dihedral_dataframe.columns)
  #print(available_calculation_list)
  for condition in ["exclude", "include"]: # the function immediately returns False if any part fails, so exclude comes first no matter what the order is in the YAML config
    for attribute in target_json[condition]:
      if attribute in available_calculation_list:
        for func_name in target_json[condition][attribute]:
          func = getattr(pd.Series, func_name)
          attribute_range_min = target_json[condition][attribute][func_name][0]
          attribute_range_max = target_json[condition][attribute][func_name][1]
          attribute_value = func(dihedral_dataframe[attribute][dihedral_dataframe["residue_number"].isin([*range(match_object.span()[0] + 1, match_object.span()[1] + 2)])])
          #print(attribute_range_min) 
          #print(attribute_value) 
          #print(attribute_range_max)
          if attribute_range_min <= attribute_value <= attribute_range_max:
            if condition == "exclude":
              print(f"Failed because the {attribute} was {attribute_value} and needed to be between {attribute_range_min} and {attribute_range_max}")
              return False
          else:
            if condition == "include":
              print(f"Failed because the {attribute} was {attribute_value} and needed to be between {attribute_range_min} and {attribute_range_max}")
              return False
      elif attribute == "flank_plddt":
        if not handle_flank(target_json[condition][attribute], match_object, dihedral_dataframe):
          #print statements are in handle_flank
          return False
      elif attribute == "size":
        size_range_min = target_json[condition][attribute][0]
        size_range_max = target_json[condition][attribute][1]
        size = (match_object.span()[1] + 2) - (match_object.span()[0] + 1)
        if condition == "exclude":
          if (size_range_min <= size <= size_range_max):
            print(f"Failed because the size was {size} and couldn't be between {size_range_min} and {size_range_max}")
            return False
        if condition == "include":
          if not (size_range_min <= size <= size_range_max):
            print(f"Failed because the size was {size} and needed to be between {size_range_min} and {size_range_max}")
            return False
            

  return True

def attribute_calculations(config_add_attr, residue_range, poly_dataframe):
    """returns a list to be column values of calculated attributes of hit sequences based on the "additional_attributes" section of the YAML config file.

    Keyword arguments:
    config_add_attr -- dictionary calculated attributes to include from the "additional_attributes" section of the config YAML
    residue_range -- a tuple of the residue range over which to calculate the value
    poly_dataframe -- a dataframe with all per-residue values needed for the attribute calculations
    """
    pass

def parse_pdb_header_custom(pdb_file):
  """This function will handle cases where the accession id of a protein is 10 characters long.
  In this case, ProDy will fail to parse the header.
  This function will return a dictionary identical to what would be in meta_dictionary.
  """
  #database, idcode, accession, full_name, full_title, depo_date, fragment, fragment_offset, organism_scientific, organism_taxid
  meta_dictionary = {"database": None,
                     "id_code": None,
                     "accession": None,
                     "full_name": None,
                     "full_title": None,
                     "depo_date": None,
                     "fragment": None,
                     "fragment_offset": None,
                     "organism_scientific": None,
                     "organism_taxid": None}
  DBREF = {
    "GB": "GenBank",
    "PDB": "PDB",
    "UNP": "UniProt",
    "NORINE": "Norine",
    "UNIMES": "UNIMES",
    "EMDB": "EMDB"
  }

  if pdb_file.endswith(".gz"):
    fileHandle = gzip.open(pdb_file, "rt")
  else:
    fileHandle = open(pdb_file)
  title_list = []
  for line in fileHandle:
    if line.startswith("HEADER"):
      #last portion of HEADER line
      meta_dictionary["depo_date"] = line.strip().split()[-1]

    if line.startswith("TITLE"):
      #Remove starting stuff, which includes an int if not the first row,
      #put in list, join at the end
      if len(title_list) > 0:
        title_list.append(" ".join(line.strip().split()[2:]))
      else:
        title_list.append(" ".join(line.strip().split()[1:]))

    if line.startswith("COMPND"):
      #if "MOL_ID" in line:
        #the pdb definition says this is correct, but our usage of 'fragment' is different
        #split on whitespace, get last element, remove semicolon
        #meta_dictionary["fragment"] = line.strip().split()[-1][:-1]
      if "MOLECULE" in line:
        #split on colon and get everything after MOLECULE:, 
        #join back together with colon in case one appears in name
        #remove bordering whitespace and trailing semicolon
        meta_dictionary["full_name"] = ":".join(line.split(":")[1:]).strip(" ;\n")

    if line.startswith("SOURCE"):
      if "ORGANISM_SCIENTIFIC" in line:
        #split on colon to get everything after ORGANISM_SCIENTIFIC:,
        #join together with colon, remove trailing semicolon and any bordering whitespace
        meta_dictionary["organism_scientific"] = ":".join(line.strip().split(":")[1:]).strip(" ;\n")
      if "ORGANISM_TAXID" in line:
        meta_dictionary["organism_taxid"] = ":".join(line.strip().split(":")[1:]).strip(" ;\n")

    if line.startswith("DBREF"):
      line = line.strip().split()
      meta_dictionary["database"] = DBREF[line[5]]
      meta_dictionary["accession"] = line[6]
      meta_dictionary["id_code"] = line[7]
      meta_dictionary["fragment_offset"] = int(line[8]) - 1

  #This calculation works for pdbs from the alphafold db, where they shift the start of a fragment by 200aa.
  #This might need to be changed if a different source is used.
  meta_dictionary["fragment"] = (meta_dictionary["fragment_offset"] // 200) + 1
  meta_dictionary["full_title"] = " ".join(title_list)


  #print(meta_dictionary)

  fileHandle.close()
  return meta_dictionary

def create_model_metainfo(pdb_file):
    """returns a ProDy model and a dictionary of meta information about a protein, taken from the PDB file header. Also returns the amino acid sequence as a list of 1-letter codes, and a Pandas dataframe of atomic coordinates.

    Keyword arguments:
    pdb_file -- the PDB structure file
    """
    meta_dictionary = None
    if len(pdb_file.split("-")[1]) == 10:
      #if the accesssion is 10 characters (allowed since ~2014), ProDy will fail to parse the header
      meta_dictionary = parse_pdb_header_custom(pdb_file)
      model = prody.parsePDB(pdb_file, header=False, model=1, meta=True)
    else:
      try:
        model, pdb_head = prody.parsePDB(pdb_file, header=True, model=1, meta=True)
      except Exception:
        #print(traceback.format_exc())
        print("Prody failed, skipping this protein")
        return None
    # using BioPandas to get matrix of PDB values and convert sequence to single letter
    af_object = PandasPdb().read_pdb(pdb_file).df["ATOM"]
    af_sequence = list(PandasPdb().read_pdb(pdb_file).amino3to1()["residue_name"])

    # decompress if needed
    if pdb_file.endswith(".gz") == True:
        uncompressed_pdb_file = gzip.open(pdb_file, "rt")
        bio_pdb = p.get_structure("XXXX", uncompressed_pdb_file)
    else:
        bio_pdb = p.get_structure("XXXX", pdb_file)

    if meta_dictionary is None:
      meta_dictionary = {
          "database": pdb_head["polymers"][0].dbrefs[0].database,
          "id_code": pdb_head["polymers"][0].dbrefs[0].idcode,
          "accession": pdb_head["polymers"][0].dbrefs[0].accession,
          "full_name": pdb_head["polymers"][0].name,
          "full_title": pdb_head["title"],
          "depo_date": pdb_head["deposition_date"],
          #This calculation works for pdbs from the alphafold db, where they shift the start of a fragment by 200aa.
          #This might need to be changed if a different source is used.
          "fragment": int((((pdb_head["polymers"][0].dbrefs[0].first[2] - pdb_head["polymers"][0].dbrefs[0].first[0]) / 200)) + 1),
          "fragment_offset": pdb_head["polymers"][0].dbrefs[0].first[2] - pdb_head["polymers"][0].dbrefs[0].first[0],
          "organism_scientific": bio_pdb.header["source"]["1"]["organism_scientific"].upper(),
          "organism_taxid": bio_pdb.header["source"]["1"]["organism_taxid"]
          }
    #print(meta_dictionary)
    #pdb_file.close()

    return model, meta_dictionary, af_object, af_sequence
